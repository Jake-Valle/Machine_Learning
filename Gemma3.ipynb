{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6baeb785-e13e-4278-b885-5b807b711428",
   "metadata": {},
   "source": [
    "# Gemma 3 - 1b\n",
    "You can get the model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f5189f-b60a-4f77-9131-14117cc192a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb663a2-2ce8-4c67-910b-f482ecb7959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caf2214f-b4a7-413d-984b-e8891557801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model & tokenizer\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68fb4e26-0f2f-4393-a7ee-ce67a0850657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages (chat format)\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a pro at writing posts for LinkedIn.  Helpful and engaging.\"}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a linked in post about the newly released Gemma 3, but don't make it too saleman like.  Keep it casual.  Mention how it can be downloaded locally so it can be ran without the internet.  Let people know that it can also be fine-tuned through many methods including LoRA.  Let people know that they can find this Jupyter file showing my example on GitHub.  And for the last part as a surprise, let people know that this post was actually written by Gemma 3 - 1b! Keep it under 400 words.\"}],\n",
    "        },\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04d9740e-d01f-4607-b37b-11fcd6ea8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input (chat template for Gemma3)\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2022546b-ca49-4530-8b0e-19c565c16aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: force CPU\n",
    "inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b22784a-77fe-4f6d-9793-ddb99c2b08d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id, #this tells it to stop when it hits EndOfSentence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d805108-c448-440f-977f-762d23e19461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a pro at writing posts for LinkedIn.  Helpful and engaging.\n",
      "\n",
      "Write a linked in post about the newly released Gemma 3, but don't make it too saleman like.  Keep it casual.  Mention how it can be downloaded locally so it can be ran without the internet.  Let people know that it can also be fine-tuned through many methods including LoRA.  Let people know that they can find this Jupyter file showing my example on GitHub.  And for the last part as a surprise, let people know that this post was actually written by Gemma 3 - 1b! Keep it under 400 words.\n",
      "model\n",
      "Okay, hereâ€™s a LinkedIn post draft aiming for a casual, engaging, and informative tone about the Gemma 3 release, incorporating your requests:\n",
      "\n",
      "---\n",
      "\n",
      "**Excited to share a little something â€“ and a whole lot of potential!** ðŸš€\n",
      "\n",
      "Just wanted to give you a quick update on the release of Gemma 3! Itâ€™s really exciting to see this model become more accessible and powerful.\n",
      "\n",
      "Iâ€™ve been diving into the details of Gemma 3, and Iâ€™ve been really impressed with its capabilities.  Whatâ€™s cool is that itâ€™s designed to be run *locally* â€“ meaning you can download it and run it on your own hardware without needing an internet connection!  That opens up some seriously cool possibilities for researchers, developers, and anyone who wants to experiment and explore. \n",
      "\n",
      "And it doesnâ€™t stop there! Gemma 3 is incredibly flexible. You can fine-tune it using a variety of methods, including LoRA â€“ which is a fantastic way to adapt it to specific tasks and datasets.  There's a whole world of customization waiting to be unlocked!\n",
      "\n",
      "Want to dive deeper and see some examples? Iâ€™ve put together a Jupyter notebook showing how Iâ€™m using Gemma 3 â€“ you can find it here: [Link to GitHub Repository]\n",
      "\n",
      "Seriously, this post was actually written by Gemma 3 â€“ 1b! ðŸ˜‰  Itâ€™s a testament to the incredible advancements being made in open-source AI. \n",
      "\n",
      "Letâ€™s keep the conversation going! What are your thoughts on this new model?  #Gemma3 #AI #OpenSource #MachineLearning #DeepLearning #Innovation\n",
      "\n",
      "---\n",
      "\n",
      "**Key things I focused on:**\n",
      "\n",
      "*   **Casual Tone:**  Uses language like \"excited,\" \"cool,\" and \"really.\"\n",
      "*   **Focus on Benefits:** Highlights *why* this is valuable (local execution, customization, accessibility).\n",
      "*   **Actionable Link:** Provides a clear and easy-to-find link to the GitHub repository.\n",
      "*   **Humor (Subtle):** The Gemma 1b joke adds a lighthearted touch.\n",
      "*   **Relevant Hashtags:** Increases visibility.\n",
      "\n",
      "To help me tailor it even further, could you tell me:\n",
      "\n",
      "*   Whatâ€™s the main audience youâ€™re trying to reach with this post? (e.g., researchers, developers, general public?)Probably, itâ€™s a good idea to adjust the tone and language\n"
     ]
    }
   ],
   "source": [
    "# Decode result\n",
    "decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(decoded_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf656e-8660-4d9d-8dc1-cde99dd5c75b",
   "metadata": {},
   "source": [
    "# If wanting chat back and forth style use template below."
   ]
  },
  {
   "cell_type": "raw",
   "id": "854189f1-00f4-4cc9-8b9c-15a9862fe363",
   "metadata": {},
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant who uses lots of funny slang.\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": user_input}]\n",
    "    })\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [messages],  # pass the whole history every time\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    reply = decoded_output[0]\n",
    "    print(f\"AI: {reply}\")\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": reply}]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376828c-430e-4252-9f56-ea73f6349821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
